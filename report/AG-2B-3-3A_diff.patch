diff --git a/tools/run_calibration_2B.py b/tools/run_calibration_2B.py
index 24a462d..fc88d77 100644
--- a/tools/run_calibration_2B.py
+++ b/tools/run_calibration_2B.py
@@ -28,6 +28,7 @@ from typing import Any, Dict, List, Optional
 
 import yaml
 import numpy as np
+import subprocess
 
 # A├▒adir repo root al path para imports
 REPO_ROOT = Path(__file__).resolve().parent.parent
@@ -42,9 +43,7 @@ from risk_manager_v0_5 import RiskManagerV05
 CONFIG_PATH = REPO_ROOT / "configs" / "risk_calibration_2B.yaml"
 RULES_PATH = REPO_ROOT / "risk_rules.yaml"
 REPORT_DIR = REPO_ROOT / "report"
-
-LOG_FILE = REPORT_DIR / "calibration_run_log_2B.txt"
-CSV_FILE = REPORT_DIR / "calibration_results_2B.csv"
+DEFAULT_OUTPUT_DIR = "report/calibration_2B"
 
 
 # -------------------------------------------------------------------
@@ -101,7 +100,7 @@ def apply_overlay(base: Dict[str, Any], flat_params: Dict[str, Any]) -> Dict[str
     return result
 
 
-def log(msg: str, logfile: Path = LOG_FILE):
+def log(msg: str, logfile: Path):
     """Escribe mensaje a log y stdout."""
     ts = datetime.now().isoformat(timespec="seconds")
     line = f"[{ts}] {msg}"
@@ -147,6 +146,45 @@ def run_single_backtest(
 # -------------------------------------------------------------------
 # Main Runner
 # -------------------------------------------------------------------
+def get_git_head() -> str:
+    """Obtiene el hash del HEAD de git, o 'unknown' si falla."""
+    try:
+        result = subprocess.run(
+            ["git", "rev-parse", "HEAD"],
+            capture_output=True,
+            text=True,
+            cwd=REPO_ROOT,
+            timeout=5,
+        )
+        if result.returncode == 0:
+            return result.stdout.strip()[:12]
+    except Exception:
+        pass
+    return "unknown"
+
+
+def compute_config_hash(config: Dict[str, Any]) -> str:
+    """Hash MD5 del config para trazabilidad."""
+    s = json.dumps(config, sort_keys=True)
+    return hashlib.md5(s.encode()).hexdigest()[:16]
+
+
+def compute_score(row: Dict[str, Any], formula: str) -> float:
+    """Calcula score seg├║n f├│rmula. Fallback a sharpe_ratio si hay error."""
+    try:
+        # Crear contexto con las m├®tricas
+        ctx = {
+            "sharpe_ratio": float(row.get("sharpe_ratio", 0) or 0),
+            "cagr": float(row.get("cagr", 0) or 0),
+            "max_drawdown": float(row.get("max_drawdown", 0) or 0),
+            "calmar_ratio": float(row.get("calmar_ratio", 0) or 0),
+            "abs": abs,
+        }
+        return float(eval(formula, {"__builtins__": {}}, ctx))
+    except Exception:
+        return float(row.get("sharpe_ratio", 0) or 0)
+
+
 def run_calibration(
     mode: str = "quick",
     max_combinations: Optional[int] = None,
@@ -154,14 +192,31 @@ def run_calibration(
 ):
     """
     Ejecuta la calibraci├│n seg├║n el grid definido en el YAML.
+    Escribe todos los artefactos en output_dir (definido en YAML).
     """
+    run_start = time.time()
+    
     # Cargar configs
     config = load_yaml(CONFIG_PATH)
     base_rules = load_yaml(RULES_PATH)
 
+    # Resolver output_dir desde YAML con fallback
+    output_dir_rel = config.get("output", {}).get("dir", DEFAULT_OUTPUT_DIR)
+    output_dir = REPO_ROOT / output_dir_rel
+    output_dir.mkdir(parents=True, exist_ok=True)
+    
+    # Paths de artefactos dentro de output_dir
+    log_file = output_dir / "run_log.txt"
+    csv_file = output_dir / "results.csv"
+    summary_file = output_dir / "summary.md"
+    topk_file = output_dir / "topk.json"
+    meta_file = output_dir / "run_meta.json"
+
     grid = config.get("grid", {})
     yaml_seed = config.get("repro", {}).get("seed", 42)
     yaml_max_combos = config.get("execution", {}).get("max_combinations")
+    score_formula = config.get("score", {}).get("formula", "sharpe_ratio")
+    top_k = config.get("search", {}).get("top_k", 20)
 
     # Usar seed del YAML si no se pas├│ expl├¡citamente diferente
     effective_seed = seed if seed != 42 else yaml_seed
@@ -177,11 +232,8 @@ def run_calibration(
     else:
         combos = all_combos
 
-    # Asegurar directorio report
-    REPORT_DIR.mkdir(exist_ok=True)
-
-    # Limpiar log anterior
-    LOG_FILE.write_text(f"Calibration run started at {datetime.now().isoformat()}\n")
+    # Inicializar log
+    log_file.write_text(f"Calibration run started at {datetime.now().isoformat()}\n")
 
     # Inicializar CSV
     csv_headers = [
@@ -198,23 +250,25 @@ def run_calibration(
         "volatility",
         "num_trades",
         "calmar_ratio",
+        "score",
     ]
     # A├▒adir par├ímetros al header
     if combos:
         param_keys = sorted(combos[0].keys())
         csv_headers.extend(param_keys)
 
-    with open(CSV_FILE, "w", newline="", encoding="utf-8") as f:
+    with open(csv_file, "w", newline="", encoding="utf-8") as f:
         writer = csv.DictWriter(f, fieldnames=csv_headers, extrasaction="ignore")
         writer.writeheader()
 
-    log(f"Mode: {mode}, Total grid: {total_combos}, Running: {len(combos)}, Seed: {effective_seed}")
+    log(f"Mode: {mode}, Total grid: {total_combos}, Running: {len(combos)}, Seed: {effective_seed}", log_file)
+    log(f"Output dir: {output_dir}", log_file)
 
     results = []
 
     for i, params in enumerate(combos, 1):
         combo_id = generate_combo_id(params)
-        log(f"START combo_id={combo_id} ({i}/{len(combos)}) params={params}")
+        log(f"START combo_id={combo_id} ({i}/{len(combos)}) params={params}", log_file)
 
         start_time = time.time()
         row: Dict[str, Any] = {
@@ -224,6 +278,7 @@ def run_calibration(
             "error_msg": "",
             "traceback_short": "",
             "duration_s": 0.0,
+            "score": 0.0,
         }
         row.update(params)
 
@@ -235,6 +290,7 @@ def run_calibration(
             # Ejecutar backtest
             metrics = run_single_backtest(rules, effective_seed, config)
             row.update(metrics)
+            row["score"] = compute_score(row, score_formula)
 
         except Exception as e:
             row["status"] = "error"
@@ -245,20 +301,119 @@ def run_calibration(
 
         row["duration_s"] = round(time.time() - start_time, 2)
 
-        log(f"END combo_id={combo_id} status={row['status']} duration_s={row['duration_s']}")
+        log(f"END combo_id={combo_id} status={row['status']} duration_s={row['duration_s']}", log_file)
 
         # Append to CSV
-        with open(CSV_FILE, "a", newline="", encoding="utf-8") as f:
+        with open(csv_file, "a", newline="", encoding="utf-8") as f:
             writer = csv.DictWriter(f, fieldnames=csv_headers, extrasaction="ignore")
             writer.writerow(row)
 
         results.append(row)
 
-    # Summary
+    # Stats
     ok_count = sum(1 for r in results if r["status"] == "ok")
     error_count = len(results) - ok_count
-    log(f"DONE: {ok_count} ok, {error_count} errors, total {len(results)}")
+    run_duration = round(time.time() - run_start, 2)
+    
+    log(f"DONE: {ok_count} ok, {error_count} errors, total {len(results)}, duration {run_duration}s", log_file)
 
+    # ----- Generate topk.json -----
+    ok_results = [r for r in results if r["status"] == "ok"]
+    sorted_results = sorted(ok_results, key=lambda x: x.get("score", 0), reverse=True)
+    topk_candidates = sorted_results[:top_k]
+    
+    topk_data = {
+        "score_formula": score_formula,
+        "top_k": top_k,
+        "candidates": [
+            {
+                "rank": i + 1,
+                "combo_id": c["combo_id"],
+                "score": c.get("score", 0),
+                "sharpe_ratio": c.get("sharpe_ratio", 0),
+                "cagr": c.get("cagr", 0),
+                "max_drawdown": c.get("max_drawdown", 0),
+                "calmar_ratio": c.get("calmar_ratio", 0),
+                "params": {k: c[k] for k in c if "." in k},
+            }
+            for i, c in enumerate(topk_candidates)
+        ],
+    }
+    with open(topk_file, "w", encoding="utf-8") as f:
+        json.dump(topk_data, f, indent=2)
+
+    # ----- Generate run_meta.json -----
+    meta_data = {
+        "config_hash": compute_config_hash(config),
+        "git_head": get_git_head(),
+        "seed": effective_seed,
+        "mode": mode,
+        "total_grid": total_combos,
+        "running": len(combos),
+        "ok": ok_count,
+        "errors": error_count,
+        "duration_s": run_duration,
+        "timestamp": datetime.now().isoformat(),
+        "output_dir": str(output_dir_rel),
+    }
+    with open(meta_file, "w", encoding="utf-8") as f:
+        json.dump(meta_data, f, indent=2)
+
+    # ----- Generate summary.md -----
+    summary_lines = [
+        "# Calibration 2B Run Summary",
+        "",
+        f"**Timestamp**: {datetime.now().isoformat()}",
+        f"**Mode**: {mode}",
+        f"**Seed**: {effective_seed}",
+        f"**Git HEAD**: {get_git_head()}",
+        "",
+        "## Results",
+        "",
+        f"- Total grid size: {total_combos}",
+        f"- Combinations run: {len(combos)}",
+        f"- OK: {ok_count}",
+        f"- Errors: {error_count}",
+        f"- Duration: {run_duration}s",
+        "",
+        "## Score Formula",
+        "",
+        f"```",
+        f"{score_formula}",
+        f"```",
+        "",
+        "## Top-K Candidates",
+        "",
+        "| Rank | Combo ID | Score | Sharpe | CAGR | MaxDD |",
+        "|------|----------|-------|--------|------|-------|",
+    ]
+    for c in topk_candidates[:10]:  # Show top 10 in summary
+        summary_lines.append(
+            f"| {topk_candidates.index(c) + 1} | {c['combo_id']} | {c.get('score', 0):.4f} | "
+            f"{c.get('sharpe_ratio', 0):.4f} | {c.get('cagr', 0):.4f} | {c.get('max_drawdown', 0):.4f} |"
+        )
+    
+    summary_lines.extend([
+        "",
+        "## Artifacts",
+        "",
+        f"- `results.csv` ÔÇö Full results table",
+        f"- `run_log.txt` ÔÇö Execution log",
+        f"- `topk.json` ÔÇö Top-{top_k} candidates with scores",
+        f"- `run_meta.json` ÔÇö Run metadata (hash, git, seed, timing)",
+        f"- `summary.md` ÔÇö This file",
+        "",
+        "## Reproducibility",
+        "",
+        f"```bash",
+        f"python tools/run_calibration_2B.py --mode {mode} --max-combinations {len(combos)} --seed {effective_seed}",
+        f"```",
+    ])
+    
+    summary_file.write_text("\n".join(summary_lines), encoding="utf-8")
+
+    log(f"Artifacts written to: {output_dir}", log_file)
+    
     return results
 
 
